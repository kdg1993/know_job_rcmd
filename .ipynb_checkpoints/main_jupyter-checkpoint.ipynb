{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d0029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Make new submission file <False>\n",
      "\n",
      "Index mismatch between train and test\n",
      "2017\n",
      "Train - Test :  set()\n",
      "Test - Train :  set()\n",
      "\n",
      "Index mismatch between train and test\n",
      "2018\n",
      "Train - Test :  {14405, 12966, 17381, 15976, 13866, 9521, 11347, 11380, 9973, 13142, 17811, 17720, 17977, 14591}\n",
      "Test - Train :  {17856, 10273, 11522, 16872, 10384, 11537, 14928, 16018, 13492, 12245, 17971}\n",
      "\n",
      "Index mismatch between train and test\n",
      "2019\n",
      "Train - Test :  {21265, 24914, 21348, 21503}\n",
      "Test - Train :  {25312, 20113, 22855}\n",
      "\n",
      "Index mismatch between train and test\n",
      "2020\n",
      "Train - Test :  set()\n",
      "Test - Train :  set()\n",
      "\n",
      "Index mismatch between test and submission\n",
      "Test - submission :  set()\n",
      "submission - Test :  set()\n",
      "\n",
      "Column mismatch between train and test\n",
      "2017\n",
      "Symmetric difference :  {'knowcode'}\n",
      "\n",
      "Column mismatch between train and test\n",
      "2018\n",
      "Symmetric difference :  {'knowcode'}\n",
      "\n",
      "Column mismatch between train and test\n",
      "2019\n",
      "Symmetric difference :  {'knowcode'}\n",
      "\n",
      "Column mismatch between train and test\n",
      "2020\n",
      "Symmetric difference :  {'knowcode'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-30 23:37:31,593]\u001b[0m A new study created in memory with name: rf_param_opt\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:38:06,345]\u001b[0m Trial 0 finished with value: 0.5482032805872811 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 13, 'max_features': 0.9556428757689246, 'n_jobs': 9, 'random_state': 42}. Best is trial 0 with value: 0.5482032805872811.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:38:31,402]\u001b[0m Trial 1 finished with value: 0.578897678666294 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 23, 'max_features': 0.6387926357773329, 'n_jobs': 9, 'random_state': 42}. Best is trial 1 with value: 0.578897678666294.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:38:36,649]\u001b[0m Trial 2 finished with value: 0.22079112835265424 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 7, 'max_features': 0.2403950683025824, 'n_jobs': 9, 'random_state': 42}. Best is trial 1 with value: 0.578897678666294.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:38:46,932]\u001b[0m Trial 3 finished with value: 0.006771010129166415 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 4, 'max_features': 0.8795585311974417, 'n_jobs': 9, 'random_state': 42}. Best is trial 1 with value: 0.578897678666294.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:39:15,345]\u001b[0m Trial 4 finished with value: 0.5756402169956866 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 19, 'max_features': 0.737265320016441, 'n_jobs': 9, 'random_state': 42}. Best is trial 1 with value: 0.578897678666294.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:39:24,885]\u001b[0m Trial 5 finished with value: 0.0008247849374014777 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 3, 'max_features': 0.9729188669457949, 'n_jobs': 9, 'random_state': 42}. Best is trial 1 with value: 0.578897678666294.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:39:37,788]\u001b[0m Trial 6 finished with value: 0.5815412678903403 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 26, 'max_features': 0.29110519961044856, 'n_jobs': 9, 'random_state': 42}. Best is trial 6 with value: 0.5815412678903403.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:39:44,083]\u001b[0m Trial 7 finished with value: 0.3322975927195949 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 8, 'max_features': 0.2650640588680905, 'n_jobs': 9, 'random_state': 42}. Best is trial 6 with value: 0.5815412678903403.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:40:03,866]\u001b[0m Trial 8 finished with value: 0.5443355751660106 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 11, 'max_features': 0.5722807884690141, 'n_jobs': 9, 'random_state': 42}. Best is trial 6 with value: 0.5815412678903403.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:40:19,216]\u001b[0m Trial 9 finished with value: 0.5802053545939869 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 15, 'max_features': 0.36210622617823773, 'n_jobs': 9, 'random_state': 42}. Best is trial 6 with value: 0.5815412678903403.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:40:25,110]\u001b[0m Trial 10 finished with value: 0.5438673823226455 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 30, 'max_features': 0.10539746466023536, 'n_jobs': 9, 'random_state': 42}. Best is trial 6 with value: 0.5815412678903403.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:40:41,623]\u001b[0m Trial 11 finished with value: 0.5823832209096145 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 24, 'max_features': 0.3909250824887194, 'n_jobs': 9, 'random_state': 42}. Best is trial 11 with value: 0.5823832209096145.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:40:59,133]\u001b[0m Trial 12 finished with value: 0.5835504838589529 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 26, 'max_features': 0.42136735731892716, 'n_jobs': 9, 'random_state': 42}. Best is trial 12 with value: 0.5835504838589529.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:41:17,373]\u001b[0m Trial 13 finished with value: 0.5885845823146763 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 0.44113729931010986, 'n_jobs': 9, 'random_state': 42}. Best is trial 13 with value: 0.5885845823146763.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:41:36,910]\u001b[0m Trial 14 finished with value: 0.5931648403631726 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 19, 'max_features': 0.4697364986990469, 'n_jobs': 9, 'random_state': 42}. Best is trial 14 with value: 0.5931648403631726.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:41:57,178]\u001b[0m Trial 15 finished with value: 0.5892609472866568 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 18, 'max_features': 0.49595428407545405, 'n_jobs': 9, 'random_state': 42}. Best is trial 14 with value: 0.5931648403631726.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:42:23,740]\u001b[0m Trial 16 finished with value: 0.5780287760982542 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 18, 'max_features': 0.6754888628066481, 'n_jobs': 9, 'random_state': 42}. Best is trial 14 with value: 0.5931648403631726.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:42:44,634]\u001b[0m Trial 17 finished with value: 0.5874533595032435 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 15, 'max_features': 0.5187440347971067, 'n_jobs': 9, 'random_state': 42}. Best is trial 14 with value: 0.5931648403631726.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:43:14,962]\u001b[0m Trial 18 finished with value: 0.5681729251238703 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 22, 'max_features': 0.7908633979307119, 'n_jobs': 9, 'random_state': 42}. Best is trial 14 with value: 0.5931648403631726.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:43:36,658]\u001b[0m Trial 19 finished with value: 0.5891706207745984 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 17, 'max_features': 0.5338162859561032, 'n_jobs': 9, 'random_state': 42}. Best is trial 14 with value: 0.5931648403631726.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:43:42,930]\u001b[0m Trial 20 finished with value: 0.5235519219496684 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 11, 'max_features': 0.14497274824543388, 'n_jobs': 9, 'random_state': 42}. Best is trial 14 with value: 0.5931648403631726.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:44:03,785]\u001b[0m Trial 21 finished with value: 0.5941322274914277 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 17, 'max_features': 0.5258825027573261, 'n_jobs': 9, 'random_state': 42}. Best is trial 21 with value: 0.5941322274914277.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:44:23,694]\u001b[0m Trial 22 finished with value: 0.5923047534282448 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 21, 'max_features': 0.48058645124131716, 'n_jobs': 9, 'random_state': 42}. Best is trial 21 with value: 0.5941322274914277.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:44:48,540]\u001b[0m Trial 23 finished with value: 0.5789243098909611 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 21, 'max_features': 0.6268581012540072, 'n_jobs': 9, 'random_state': 42}. Best is trial 21 with value: 0.5941322274914277.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:45:03,244]\u001b[0m Trial 24 finished with value: 0.5805885865448126 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 15, 'max_features': 0.3386678999591979, 'n_jobs': 9, 'random_state': 42}. Best is trial 21 with value: 0.5941322274914277.\u001b[0m\n",
      "\u001b[32m[I 2021-12-30 23:45:22,295]\u001b[0m Trial 25 finished with value: 0.5867089496366832 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'max_depth': 25, 'max_features': 0.45941629523091054, 'n_jobs': 9, 'random_state': 42}. Best is trial 21 with value: 0.5941322274914277.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%% Import packages\n",
    "from optuna import Trial\n",
    "\n",
    "#%% Custom functions\n",
    "def objectiveRF(trial: Trial, X, y, X_val, y_val):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [300]),\n",
    "        'criterion':trial.suggest_categorical('criterion', ['entropy']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        # 'max_features': trial.suggest_categorical(\n",
    "        #     'max_features', [None, 'sqrt', 'log2']),\n",
    "        'max_features': trial.suggest_float('max_features', 0.1, 1),\n",
    "        'n_jobs': trial.suggest_categorical('n_jobs', [cpu_use]),\n",
    "        'random_state': trial.suggest_categorical('random_state', [42]),\n",
    "        }\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    rf_model = model.fit(X, y)\n",
    "    \n",
    "    score = f1_score(y_val, rf_model.predict(X_val), average='macro')\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def get_rf_optuna(X_tr, y_tr, X_val, y_val, n_trial):\n",
    "    study = optuna.create_study(\n",
    "        study_name='rf_param_opt',\n",
    "        direction='maximize', \n",
    "        sampler=TPESampler(seed=42)\n",
    "        )\n",
    "    \n",
    "    study.optimize(lambda trial: objectiveRF(\n",
    "        trial, X_tr, y_tr, X_val, y_val),\n",
    "        n_trials=n_trial)\n",
    "    \n",
    "    best_rf = RandomForestClassifier(**study.best_params).fit(\n",
    "        pd.concat([X_tr, X_val], axis=0),\n",
    "        pd.concat([y_tr, y_val], axis=0),\n",
    "        )\n",
    "    \n",
    "    return best_rf, study.best_value, study\n",
    "\n",
    "\n",
    "def fix_random_seed(seed=42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import os\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#%% Fix random seed\n",
    "fix_random_seed()\n",
    "\n",
    "#%% Main script\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #% Import packages\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import pickle\n",
    "    import itertools as it\n",
    "    import optuna\n",
    "    import joblib\n",
    "    \n",
    "    from matplotlib import pyplot as plt\n",
    "    from copy import deepcopy\n",
    "    from datetime import datetime\n",
    "    from tqdm import tqdm\n",
    "    from glob import glob\n",
    "    from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from scipy.stats import hmean\n",
    "    from optuna import visualization\n",
    "    from optuna.samplers import TPESampler    \n",
    "    from multiprocessing import cpu_count\n",
    "    \n",
    "    #%% Overall settings\n",
    "    run_new_submission = False\n",
    "    print('\\nMake new submission file <{}>'.format(\n",
    "        run_new_submission))\n",
    "    \n",
    "    cpu_use = int(3*cpu_count()/4)\n",
    "\n",
    "    #%% Load data\n",
    "    years = [2017, 2018, 2019, 2020]\n",
    "    \n",
    "    df_smp_subm = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "    path_tr = sorted(glob('./data/train/*'))\n",
    "    path_test = sorted(glob('./data/test/*'))\n",
    "    \n",
    "    dict_tr = {y:pd.read_csv(p, low_memory=False) \n",
    "               for y, p in zip(years, path_tr)}\n",
    "    dict_test = {y:pd.read_csv(p, low_memory=False) \n",
    "               for y, p in zip(years, path_test)}\n",
    "    \n",
    "    #%% Check data index match\n",
    "    # Check index mismatch between train and test\n",
    "    for y in years:\n",
    "        i_tr = set(dict_tr[y].idx)\n",
    "        i_test = set(dict_test[y].idx)\n",
    "        \n",
    "        print('\\nIndex mismatch between train and test')\n",
    "        print(y)\n",
    "        print('Train - Test : ', i_tr.difference(i_test))\n",
    "        print('Test - Train : ', i_test.difference(i_tr))\n",
    "\n",
    "    # Check index mismatch between test and submission\n",
    "    idx_test = pd.concat([dict_test[k].idx for k in dict_test.keys()], axis=0)\n",
    "    print('\\nIndex mismatch between test and submission')\n",
    "    print('Test - submission : ', set(idx_test) - set(df_smp_subm.idx))\n",
    "    print('submission - Test : ', set(df_smp_subm.idx) - set(idx_test))  \n",
    "    \n",
    "    #%% Check data column match\n",
    "    # Check column mismatch between train and test\n",
    "    for year in years:\n",
    "        c_tr = set(dict_tr[year].columns)\n",
    "        c_test = set(dict_test[year].columns)\n",
    "        \n",
    "        print('\\nColumn mismatch between train and test')\n",
    "        print(year)\n",
    "        print('Symmetric difference : ', c_tr.symmetric_difference(c_test))\n",
    "    \n",
    "    #%% Data preprocessing\n",
    "    # Set index\n",
    "    dict_tr = {k:v.set_index('idx') for k, v in dict_tr.items()}\n",
    "    dict_test = {k:v.set_index('idx') for k, v in dict_test.items()}    \n",
    "    \n",
    "    # Fill empty elements\n",
    "    dict_tr = {k:v.replace(' ', -1) for k, v in dict_tr.items()}\n",
    "    dict_test = {k:v.replace(' ', -1) for k, v in dict_test.items()}\n",
    "    \n",
    "    #%% Make label encoder for each years of datasets\n",
    "    # Train set label encoding\n",
    "    dict_encoder = {}\n",
    "    \n",
    "    for y, df in dict_tr.items():\n",
    "        encoder_pack = {}\n",
    "        \n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].map(float)\n",
    "                df[col] = df[col].map(int)\n",
    "            except:\n",
    "                encoder = LabelEncoder()\n",
    "                df[col] = df[col].map(str)\n",
    "                df[col] = encoder.fit_transform(df[col])\n",
    "                encoder_pack[col] = encoder\n",
    "                \n",
    "        dict_encoder[y] = encoder_pack\n",
    "                \n",
    "        \n",
    "    # Test set label encoding\n",
    "    for y, df in dict_test.items():\n",
    "        encoder_pack = dict_encoder[y]        \n",
    "        \n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].map(float)\n",
    "                df[col] = df[col].map(int)\n",
    "            except:\n",
    "                try:\n",
    "                    encoder = encoder_pack[col]\n",
    "                    df[col] = df[col].map(str)\n",
    "                    category_map = {category: idx for idx, category in\n",
    "                                    enumerate(encoder.classes_)}\n",
    "                    df[col] = df[col].apply(\n",
    "                        lambda x: category_map[x] if x in category_map else -2)\n",
    "                    # -2 indicates unseen in train set\n",
    "                except:\n",
    "                    print('\\nThere is no encoder for test set', y, col)\n",
    "                    df[col] = df[col].apply(\n",
    "                        lambda x: -3 if len(x)>=2 else x)\n",
    "                    \n",
    "    #%% (Test) Remove every feature with string value\n",
    "    '''\n",
    "    dict_tr = {k:v.drop(columns=dict_encoder[k].keys()) \n",
    "               for k, v in dict_tr.items()}\n",
    "    \n",
    "    dict_test = {k:v.drop(columns=dict_encoder[k].keys()) \n",
    "                 for k, v in dict_test.items()}\n",
    "    '''\n",
    "          \n",
    "    #%% Add values to the dataset to make it positive\n",
    "    # It is necessary for Multinomial Naive Bayes model\n",
    "    '''\n",
    "    min_val_data = 0\n",
    "    \n",
    "    for df in list_tr+list_test:\n",
    "        min_val_data = min(min_val_data, df.min().min())\n",
    "        \n",
    "    for df in list_tr+list_test:\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] -= min_val_data\n",
    "            except TypeError:\n",
    "                pass\n",
    "    '''\n",
    "    \n",
    "    #%% Train validation split (by stratify)\n",
    "    X_tr = {}\n",
    "    y_tr = {}\n",
    "    X_val = {}\n",
    "    y_val = {}\n",
    "    \n",
    "    for y, df in dict_tr.items():\n",
    "        tr, val = train_test_split(df, test_size=0.2, random_state=42,\n",
    "                                   shuffle=True, stratify=df.knowcode)\n",
    "        X_tr[y] = tr.drop(columns='knowcode')\n",
    "        y_tr[y] = tr.knowcode\n",
    "        X_val[y] = val.drop(columns='knowcode')\n",
    "        y_val[y] = val.knowcode\n",
    "    \n",
    "    #%% Train naive bayes models\n",
    "    mdl_nb = {y:GaussianNB().fit(X_tr[y], y_tr[y]) for y in years}\n",
    "    \n",
    "    #%% RandomForest hyperparameter search by optuna\n",
    "    rslt_param_opt = \\\n",
    "        {y: get_rf_optuna(X_tr[y], y_tr[y], X_val[y], y_val[y], 30)\n",
    "         for y in years}\n",
    "    \n",
    "    #%% Divide parameter optimization results\n",
    "    mdl_best = {k:v[0] for k, v in rslt_param_opt.items()}\n",
    "    dict_val_f1 = {k:v[1] for k, v in rslt_param_opt.items()}\n",
    "    rslt_opt = {k:v[2] for k, v in rslt_param_opt.items()}\n",
    "    \n",
    "    for k, v in dict_val_f1.items():\n",
    "        print('Val score of {}: {}'.format(k, round(v, 3)))\n",
    "    \n",
    "    print('Val harmonic mean score : {}'.format(round(\n",
    "        hmean([v for k, v in dict_val_f1.items()]), 3)))\n",
    "    \n",
    "    #%% Predict test set\n",
    "    y_pred = [pd.Series(index=dict_test[y].index,\n",
    "                        data=mdl_best[y].predict(dict_test[y]),\n",
    "                        name='knowcode')\n",
    "              for y in years]\n",
    "    y_pred = pd.concat(y_pred, axis=0)\n",
    "    \n",
    "    df_subm = y_pred.reset_index()\n",
    "        \n",
    "    #%% Make and Save submission\n",
    "    if run_new_submission:\n",
    "        if not os.path.exists('submission_save'):\n",
    "            os.makedirs('submission_save')\n",
    "            \n",
    "        dir_org = os.getcwd()            \n",
    "        os.chdir('submission_save')\n",
    "        cur_t = '{}_{}_{}_{}_{}'.format(datetime.now().year, \n",
    "                                        datetime.now().month,\n",
    "                                        datetime.now().day, \n",
    "                                        datetime.now().hour,\n",
    "                                        datetime.now().minute)\n",
    "        os.mkdir('submission_'+cur_t)\n",
    "        os.chdir('submission_'+cur_t)\n",
    "               \n",
    "        df_subm.to_csv('submission_time_'+cur_t+'.csv',\n",
    "                       index=False)\n",
    "        \n",
    "        try:\n",
    "            with open ('best_mdl', 'wb') as f:\n",
    "                pickle.dump(mdl_best, f)\n",
    "        except MemoryError:\n",
    "            os.remove('best_mdl')\n",
    "            print('Pickle Memory error >> Save model by joblib instead')\n",
    "            joblib.dump(mdl_best, 'best_mdl.pkl')\n",
    "            \n",
    "        with open ('param_opt_rslt', 'wb') as f:\n",
    "            pickle.dump(rslt_opt, f)\n",
    "        \n",
    "        os.chdir(dir_org)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b101c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
